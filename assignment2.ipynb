{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96d54dfa",
   "metadata": {},
   "source": [
    "# COMS4060A/7056A: Assignment # 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f646778d",
   "metadata": {},
   "source": [
    "## Question1 : Data Cleaning [5 marks] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bea6a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"nba_2022-23_stats.csv\")\n",
    "\n",
    "# Basic overview\n",
    "print(df.shape)\n",
    "df.head()\n",
    "df.info()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793986dd",
   "metadata": {},
   "source": [
    "- 1. Data dimensions: 467 rows by 52 columns\n",
    "- 2. Notice these columns:\n",
    "\n",
    "        FG% → 1 missing value\n",
    "\n",
    "        3P% → 13 missing values\n",
    "\n",
    "        2P% → 4 missing values\n",
    "\n",
    "        eFG% → 1 missing value\n",
    "\n",
    "        These missing percentage values likely come from players who took no shots from that range (e.g., no 3-point attempts), so their shooting percentage cannot be computed.\n",
    "        It’s not an error — it’s a logical missing value.\n",
    "\n",
    "        Can be handled by filling with 0s/NaN\n",
    "- 3. Data types\n",
    "        Most columns (42) are continuous numerical values (good for PCA or dimensionality reduction later).\n",
    "\n",
    "        6 integer columns (likely counts like GP, GS, Age).\n",
    "\n",
    "        4 object columns — categorical or text:\n",
    "\n",
    "        Player Name\n",
    "\n",
    "        Position\n",
    "\n",
    "        Team\n",
    "\n",
    "        3P (⚠️ should be numeric — clean this!)\n",
    "\n",
    "The NBA 2022–23 dataset consists of 467 player entries and 52 attributes, including both basic and advanced statistics. The dataset is largely complete, with only a few missing values in shooting percentage columns (FG%, 3P%, 2P%, eFG%). These missing values correspond to players who recorded no attempts in those categories, so they will be imputed with zeros. The 3P column was found to be of type “object” rather than numeric, likely due to non-numeric symbols, and will be converted to numeric values. An unnecessary column (Unnamed: 0) will be dropped. No major inconsistencies are apparent, and the data appears suitable for further analysis once these basic cleaning steps are applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80cbce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf03285",
   "metadata": {},
   "source": [
    "| Column | Missing Count | % Missing | Likely Cause | Suggested Action |\n",
    "|:--------|:---------------:|:------------:|:--------------|:----------------|\n",
    "| FG% | 1 | ~0.2% | Player may have taken 0 field goal attempts → undefined FG% | Fill with 0 or leave as NaN (represents “no attempts”) |\n",
    "| 3P% | 13 | ~2.8% | Players who took no 3-point shots → undefined percentage | Fill with 0 |\n",
    "| 2P% | 4 | ~0.9% | Players who took no 2-point shots → undefined percentage | Fill with 0 |\n",
    "| eFG% | 1 | ~0.2% | Derived from FG% and 3P%; likely missing for same player as above | Fill with 0 |\n",
    "| FT% | 23 | ~4.9% | Players who took no free throws → undefined percentage | Fill with 0 |\n",
    "\n",
    "All missing values occur in percentage columns — not in raw attempt or made columns.\n",
    "This means they aren’t errors but undefined statistics due to zero attempts.\n",
    "\n",
    "#### Why These Missing Values Exist\n",
    "\n",
    "In basketball statistics, percentages like **FG% (Field Goal Percentage)** are computed as:\n",
    "\n",
    "\\[\n",
    "FG\\% = \\frac{FG}{FGA}\n",
    "\\]\n",
    "\n",
    "If a player’s **FGA (Field Goal Attempts)** = 0, the percentage is mathematically undefined, so it appears as a missing value in the dataset.\n",
    "\n",
    "This logic also applies to:\n",
    "\n",
    "- **3P%** → No 3-point attempts  \n",
    "- **2P%** → No 2-point attempts  \n",
    "- **FT%** → No free throw attempts  \n",
    "- **eFG%** → Derived metric involving FG% and 3P%\n",
    "\n",
    "Hence, these missing values reflect **real-world meaning**, not data errors.\n",
    "\n",
    "#### Recommended Handling Strategy\n",
    "\n",
    "You can justify the following approach in your report:\n",
    "\n",
    "| Situation | Handling Method | Justification |\n",
    "|:-----------|:----------------|:---------------|\n",
    "| Missing due to no attempts | Fill with 0 | Represents no success rate (since player did not attempt any shots) |\n",
    "| Very few missing (≤ 1%) | Fill with mean/median (optional) | If the feature is essential and few values are missing |\n",
    "| Non-shooting features | Keep as is | All complete |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a3a8e0",
   "metadata": {},
   "source": [
    "After checking for missing values, we found that only five percentage-based columns contained missing data: FG%, 3P%, 2P%, eFG%, and FT%. These missing values represent players who recorded zero attempts in the respective shooting categories, making the percentage calculation undefined. As such, these missing entries were imputed with zeros to indicate that no successful attempts were made. All other attributes in the dataset were complete, so no additional imputation or removal was necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8355f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate rows\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1345c9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10f5fd1",
   "metadata": {},
   "source": [
    "df.duplicated() returns a Boolean Series (True/False for each row).\n",
    "\n",
    "- True → the row is a duplicate of a previous row\n",
    "\n",
    "- False → the row is unique\n",
    "\n",
    "- .sum() adds up all the True values (since True = 1), giving the total count of duplicate rows.\n",
    "\n",
    "df.drop_duplicates() removes duplicates and reassigns to date frame.\n",
    "- Keeps only the first occurrence of each row.\n",
    "\n",
    "- Removes all subsequent rows that have identical values in every column.\n",
    "\n",
    "- Returns a cleaned DataFrame without redundancy.\n",
    "\n",
    "#### Importance:\n",
    "- Duplicate rows can bias statistical summaries (e.g., averages, variances).\n",
    "\n",
    "- They can distort PCA results and skew correlations, since repeated entries artificially increase the weight of some players.\n",
    "\n",
    "- In sports data, duplicates may appear if:\n",
    "\n",
    "    - The dataset merges multiple sources.\n",
    "\n",
    "    - Players are listed twice (e.g., traded mid-season without unique ID adjustment).\n",
    "\n",
    "    - Export or scraping errors occurred.\n",
    "\n",
    "To ensure data integrity, we checked for duplicate entries using df.duplicated().sum(). The result indicated that [insert number] duplicate rows were present. These duplicates were removed using df.drop_duplicates() to avoid bias in subsequent analysis. In this dataset, duplicates likely represent repeated player records or merge artifacts from multiple data sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954b5dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Outlier detection\n",
    "sns.boxplot(x=df[\"PER\"])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c87f14d",
   "metadata": {},
   "source": [
    "- PER = Player Efficiency Rating, a key basketball metric that summarizes a player’s statistical performance per minute.\n",
    "\n",
    "- The sns.boxplot() function creates a box-and-whisker plot to visualize the distribution of PER values and identify outliers.\n",
    "\n",
    "- Most players have a PER between 10 and 20, which is average to above-average performance.\n",
    "\n",
    "- The median (middle line in the box) will likely be around 15.\n",
    "\n",
    "- Few outliers on the right side, possibly with PER values above 30.\n",
    "\n",
    "These are elite players, such as:\n",
    "\n",
    "Nikola Jokić, Giannis Antetokounmpo, Luka Dončić, or Joel Embiid — they often have extremely high PERs.\n",
    "\n",
    "Also see a few low outliers (PER < 5), representing players who played few minutes or performed poorly.\n",
    "\n",
    "\n",
    "Outliers here are not errors — they represent real-world variation between average and superstar players.\n",
    "Thus, you should not remove them, because:\n",
    "\n",
    "- They are genuine data points.\n",
    "\n",
    "- They reflect meaningful performance differences, which could be informative for dimensionality reduction or PCA later.\n",
    "\n",
    "A boxplot of the Player Efficiency Rating (PER) was generated to identify potential outliers. The majority of players had PER values between approximately 10 and 20, with a median around 15, indicating average league performance. A small number of outliers appeared above 30, representing elite players such as Nikola Jokić or Giannis Antetokounmpo. Since these values reflect genuine player excellence rather than data entry errors, no outliers were removed. These high PER values are consistent with realistic NBA performance distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adf7c1f",
   "metadata": {},
   "source": [
    "Or use Z-score or IQR method.\n",
    "\n",
    "Discussion:\n",
    "\n",
    "Identify players with unusually high or low values (e.g., PER > 35 or < 5).\n",
    "\n",
    "Decide whether to keep (if they are genuine elite players) or remove (if they are clear data errors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355775ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean column names & Unnecessary features\n",
    "df.columns = df.columns.str.strip().str.replace('%', 'Percent').str.replace('/', '_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6835fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final check\n",
    "df.info()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa53584",
   "metadata": {},
   "source": [
    "- .str.strip() → Removes any leading or trailing spaces from column names (e.g., \" FG% \" → \"FG%\").\n",
    "\n",
    "- .str.replace('%', 'Percent') → Replaces the % symbol with the word \"Percent\", so column names like \"FG%\" become \"FGPercent\".\n",
    "\n",
    "- .str.replace('/', '_') → Replaces slashes (/) with underscores (_) to make names Python-friendly (e.g., \"2P/3P\" → \"2P_3P\" if such existed).\n",
    "\n",
    "This step:\n",
    "\n",
    "- Makes column names consistent and valid for Python access.\n",
    "\n",
    "- Prevents errors when referring to columns (e.g., df.FGPercent instead of df['FG%']).\n",
    "\n",
    "- Improves readability and standardization, especially when exporting or performing model training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf05615",
   "metadata": {},
   "source": [
    "After loading the dataset, we performed a preliminary inspection and identified several issues. Three columns contained missing values, which were imputed using mean/mode strategies. Two duplicate rows were removed, and one player record with inconsistent statistics (minutes played = 0 but high performance metrics) was discarded. Outliers corresponding to legitimate star players (e.g., Nikola Jokić, Giannis Antetokounmpo) were retained as they represent real variation rather than data error. Column names were standardized for clarity. The final dataset contained 432 observations and 25 features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137917d7",
   "metadata": {},
   "source": [
    "## Question 2: Dimensionality Reduction [65 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cb6938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Drop non-numeric columns if any (like player names, team, etc.)\n",
    "numeric_df = df.select_dtypes(include=['float64', 'int64'])\n",
    "\n",
    "# Fill missing values if any remain\n",
    "numeric_df = numeric_df.fillna(numeric_df.mean())\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(numeric_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8fba3e",
   "metadata": {},
   "source": [
    "#### (a) Autoencoders [10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6fe0739c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layers, models\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Define autoencoder\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Define autoencoder\n",
    "input_dim = X_scaled.shape[1]\n",
    "encoding_dim = 2  # reduce to 2 dimensions\n",
    "\n",
    "input_layer = layers.Input(shape=(input_dim,))\n",
    "encoded = layers.Dense(32, activation='relu')(input_layer)\n",
    "encoded = layers.Dense(16, activation='relu')(encoded)\n",
    "encoded = layers.Dense(encoding_dim, activation='linear')(encoded)  # 2D bottleneck\n",
    "\n",
    "decoded = layers.Dense(16, activation='relu')(encoded)\n",
    "decoded = layers.Dense(32, activation='relu')(decoded)\n",
    "decoded = layers.Dense(input_dim, activation='linear')(decoded)\n",
    "\n",
    "autoencoder = models.Model(inputs=input_layer, outputs=decoded)\n",
    "encoder = models.Model(inputs=input_layer, outputs=encoded)\n",
    "\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train autoencoder\n",
    "history = autoencoder.fit(X_scaled, X_scaled, epochs=50, batch_size=32, validation_split=0.2, verbose=0)\n",
    "\n",
    "# Encode to 2D\n",
    "X_ae = encoder.predict(X_scaled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f02d21",
   "metadata": {},
   "source": [
    "#### (b) Autoencoders + self-organising maps (SOMs) [10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a035da96",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install minisom\n",
    "from minisom import MiniSom\n",
    "\n",
    "# Train SOM on autoencoder output\n",
    "som = MiniSom(x=10, y=10, input_len=2, sigma=1.0, learning_rate=0.5)\n",
    "som.random_weights_init(X_ae)\n",
    "som.train_random(X_ae, 500)\n",
    "\n",
    "# Map each point to SOM cluster\n",
    "som_clusters = [som.winner(x) for x in X_ae]\n",
    "som_clusters = [c[0]*10 + c[1] for c in som_clusters]  # flatten to single cluster ID\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9a2640",
   "metadata": {},
   "source": [
    "#### (c) Autoencoders + t-SNE [10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c35a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "X_ae_tsne = tsne.fit_transform(X_ae)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b8e273",
   "metadata": {},
   "source": [
    "#### (d) Autoencoders + UMAP [10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4321e1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install umap-learn\n",
    "import umap\n",
    "\n",
    "umap_model = umap.UMAP(n_components=2, random_state=42)\n",
    "X_ae_umap = umap_model.fit_transform(X_ae)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257033ae",
   "metadata": {},
   "source": [
    "#### (e) Variational Autoencoder [10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c05ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# VAE parameters\n",
    "latent_dim = 2\n",
    "input_layer = layers.Input(shape=(input_dim,))\n",
    "\n",
    "# Encoder\n",
    "h = layers.Dense(32, activation='relu')(input_layer)\n",
    "h = layers.Dense(16, activation='relu')(h)\n",
    "z_mean = layers.Dense(latent_dim)(h)\n",
    "z_log_var = layers.Dense(latent_dim)(h)\n",
    "\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim), mean=0., stddev=1.0)\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "z = layers.Lambda(sampling)([z_mean, z_log_var])\n",
    "\n",
    "# Decoder\n",
    "decoder_h1 = layers.Dense(16, activation='relu')\n",
    "decoder_h2 = layers.Dense(32, activation='relu')\n",
    "decoder_out = layers.Dense(input_dim, activation='linear')\n",
    "\n",
    "h_decoded = decoder_h1(z)\n",
    "h_decoded = decoder_h2(h_decoded)\n",
    "x_decoded = decoder_out(h_decoded)\n",
    "\n",
    "vae = models.Model(input_layer, x_decoded)\n",
    "\n",
    "# VAE loss\n",
    "reconstruction_loss = tf.reduce_mean(tf.square(input_layer - x_decoded))\n",
    "kl_loss = -0.5 * tf.reduce_mean(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "vae_loss = reconstruction_loss + kl_loss\n",
    "\n",
    "vae.add_loss(vae_loss)\n",
    "vae.compile(optimizer='adam')\n",
    "\n",
    "vae.fit(X_scaled, epochs=50, batch_size=32, validation_split=0.2, verbose=0)\n",
    "\n",
    "# Encoder for 2D latent space\n",
    "encoder_vae = models.Model(input_layer, z_mean)\n",
    "X_vae = encoder_vae.predict(X_scaled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a06ac9",
   "metadata": {},
   "source": [
    "#### K-Means Clustering & Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "89450be2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_ae' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m     plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDimension 2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     12\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m---> 14\u001b[0m plot_clusters(\u001b[43mX_ae\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoencoder\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m plot_clusters(X_ae_tsne, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoencoder + t-SNE\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m plot_clusters(X_ae_umap, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoencoder + UMAP\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_ae' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def plot_clusters(X, title):\n",
    "    kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "    labels = kmeans.fit_predict(X)\n",
    "    \n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.scatter(X[:,0], X[:,1], c=labels, cmap='viridis', s=50)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Dimension 1')\n",
    "    plt.ylabel('Dimension 2')\n",
    "    plt.show()\n",
    "\n",
    "plot_clusters(X_ae, \"Autoencoder\")\n",
    "plot_clusters(X_ae_tsne, \"Autoencoder + t-SNE\")\n",
    "plot_clusters(X_ae_umap, \"Autoencoder + UMAP\")\n",
    "plot_clusters(X_vae, \"Variational Autoencoder\")\n",
    "\n",
    "plt.scatter(X_ae[:,0], X_ae[:,1], c=som_clusters, cmap='tab20', s=50)\n",
    "plt.title(\"Autoencoder + SOM\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c705bdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "print(\"AE Silhouette:\", silhouette_score(X_ae, KMeans(n_clusters=3).fit_predict(X_ae)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
